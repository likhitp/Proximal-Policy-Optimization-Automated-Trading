{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Project Plan for Building an Auto Trading Bot with PPO\n",
        "\n",
        "\n",
        "#### **Phase 1: Data Acquisition**\n",
        "- **Objective:** Obtain historical stock data for TSLA from Yahoo Finance.\n",
        "- **Steps:**\n",
        "  1. Download 1-hour interval data for TSLA for the period:\n",
        "     - **Training Period:** July 1, 2023, to December 31, 2023.\n",
        "     - **Testing Period:** January 1, 2024, to June 30, 2024.\n",
        "  2. Calculate technical indicators: Moving Averages (short-term and long-term), RSI, and MACD.\n",
        "  3. Ensure that the data is cleaned and ready for use in the RL environment.\n",
        "\n",
        "#### **Phase 2: Environment Setup**\n",
        "- **Objective:** Set up the trading environment where the PPO agent will interact.\n",
        "- **Steps:**\n",
        "  1. **Define the action space:** Buy, Sell, Hold.\n",
        "  2. **Define the state space:** Include the price data (open, high, low, close, volume) and calculated indicators (Moving Averages, RSI, MACD).\n",
        "  3. **Implement the reward function:**\n",
        "     - Profit-based reward (highest weight).\n",
        "     - Risk-adjusted return (medium weight).\n",
        "     - Penalty for inactivity (lowest weight).\n",
        "  4. **Initialize the environment:** Start with $10,000 capital, no transaction costs, no minimum cash balance.\n",
        "\n",
        "#### **Phase 3: PPO Implementation**\n",
        "- **Objective:** Implement the PPO algorithm to train the agent.\n",
        "- **Steps:**\n",
        "  1. **Model Design:** Set up a neural network architecture that’s adequate for this task (e.g., using Dense layers with ReLU activation).\n",
        "  2. **PPO Configuration:**\n",
        "     - Use appropriate hyperparameters (e.g., learning rate, discount factor, clip range, batch size).\n",
        "     - Ensure that optimization techniques (like gradient clipping and early stopping) are applied to keep the training within the 1-hour limit.\n",
        "  3. **Training Loop:**\n",
        "     - Train the model on the second half of 2023 data.\n",
        "     - Save intermediate models and logs during training.\n",
        "     - Optimize the training loop for speed (consider using PyTorch or TensorFlow with GPU acceleration).\n",
        "\n",
        "#### **Phase 4: Testing and Evaluation**\n",
        "- **Objective:** Evaluate the trained PPO agent on the testing dataset.\n",
        "- **Steps:**\n",
        "  1. **Test the Model:** Run the PPO model on the first half of 2024 data.\n",
        "  2. **Record Actions:** Store each action taken (Buy, Sell, Hold) along with the associated state and reward in a DataFrame.\n",
        "  3. **Performance Metrics:** Compare the model's performance against benchmarks (e.g., buy-and-hold strategy) and calculate metrics like cumulative return, maximum drawdown, Sharpe Ratio.\n",
        "\n",
        "#### **Phase 5: Visualization and Analysis**\n",
        "- **Objective:** Provide a comprehensive analysis of the model's performance.\n",
        "- **Steps:**\n",
        "  1. **Visualize Training Metrics:** Use TensorBoard or Matplotlib to plot training loss, rewards, and other relevant metrics.\n",
        "  2. **Visualize Trading Actions:** Plot the price of TSLA over time, highlighting the Buy, Sell, and Hold actions taken by the model.\n",
        "  3. **Analyze Results:** Summarize the key findings in a report, discuss the strengths and weaknesses of the model, and provide suggestions for future improvements.\n",
        "\n",
        "#### **Phase 6: Final Delivery**\n",
        "- **Objective:** Deliver the project with clean, optimized, and well-documented code.\n",
        "- **Steps:**\n",
        "  1. **Code Review:** Ensure the code follows Python best practices (PEP8) and is well-commented.\n",
        "  2. **Deliverables:**\n",
        "     - Python scripts or Jupyter notebooks with the full implementation.\n",
        "     - DataFrame with the actions and associated data.\n",
        "     - Visualizations and a brief analysis report.\n"
      ],
      "metadata": {
        "id": "UEiGB-qoqspZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yZWz4v1NrQCu"
      }
    },
    {
      "source": [
        "!pip install ta"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tcvhzHnrQk1",
        "outputId": "06455084-428e-45e9-8584-0759da57520e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ta) (2.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.16.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=5b61da8bcef5435a6e1c85a0698b79ddccb23ef9e2d22bc87d8449f194040e20\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/67/4f/8a9f252836e053e532c6587a3230bc72a4deb16b03a829610b\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import ta\n",
        "\n",
        "# Download the data for the extended period\n",
        "start_train = \"2023-01-01\"\n",
        "end_train = \"2023-12-31\"\n",
        "start_test = \"2024-01-01\"\n",
        "end_test = \"2024-06-30\"\n",
        "\n",
        "# Fetch the training and testing data\n",
        "data_train = yf.download('TSLA', start=start_train, end=end_train, interval='1h')\n",
        "data_test = yf.download('TSLA', start=start_test, end=end_test, interval='1h')\n",
        "\n",
        "# Calculate the technical indicators for training data\n",
        "data_train['SMA_20'] = ta.trend.sma_indicator(data_train['Close'], window=20)\n",
        "data_train['SMA_50'] = ta.trend.sma_indicator(data_train['Close'], window=50)\n",
        "data_train['RSI'] = ta.momentum.rsi(data_train['Close'], window=14)\n",
        "data_train['MACD'] = ta.trend.macd_diff(data_train['Close'])\n",
        "data_train['Bollinger_Upper'], data_train['Bollinger_Lower'] = ta.volatility.bollinger_hband(data_train['Close']), ta.volatility.bollinger_lband(data_train['Close'])\n",
        "data_train['Stochastic'] = ta.momentum.stoch(data_train['High'], data_train['Low'], data_train['Close'])\n",
        "data_train['ADX'] = ta.trend.adx(data_train['High'], data_train['Low'], data_train['Close'])\n",
        "\n",
        "# Drop NaN values that may arise from indicator calculations\n",
        "data_train.dropna(inplace=True)\n",
        "\n",
        "# Calculate the technical indicators for test data\n",
        "data_test['SMA_20'] = ta.trend.sma_indicator(data_test['Close'], window=20)\n",
        "data_test['SMA_50'] = ta.trend.sma_indicator(data_test['Close'], window=50)\n",
        "data_test['RSI'] = ta.momentum.rsi(data_test['Close'], window=14)\n",
        "data_test['MACD'] = ta.trend.macd_diff(data_test['Close'])\n",
        "data_test['Bollinger_Upper'], data_test['Bollinger_Lower'] = ta.volatility.bollinger_hband(data_test['Close']), ta.volatility.bollinger_lband(data_test['Close'])\n",
        "data_test['Stochastic'] = ta.momentum.stoch(data_test['High'], data_test['Low'], data_test['Close'])\n",
        "data_test['ADX'] = ta.trend.adx(data_test['High'], data_test['Low'], data_test['Close'])\n",
        "\n",
        "# Drop NaN values that may arise from indicator calculations\n",
        "data_test.dropna(inplace=True)\n",
        "\n",
        "# Verify that the columns exist\n",
        "print(\"Training data columns:\", data_train.columns)\n",
        "print(\"Testing data columns:\", data_test.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iNkSpShqsXe",
        "outputId": "cb0ccf99-5010-4693-b426-6d16aa80bbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data columns: Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'SMA_20',\n",
            "       'SMA_50', 'RSI', 'MACD', 'Bollinger_Upper', 'Bollinger_Lower',\n",
            "       'Stochastic', 'ADX'],\n",
            "      dtype='object')\n",
            "Testing data columns: Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'SMA_20',\n",
            "       'SMA_50', 'RSI', 'MACD', 'Bollinger_Upper', 'Bollinger_Lower',\n",
            "       'Stochastic', 'ADX'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environments after confirming the indicators are in the DataFrame\n",
        "env = TradingEnv(data_train)\n",
        "test_env = TradingEnv(data_test)\n"
      ],
      "metadata": {
        "id": "7ZEBftD1E2V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "\n",
        "class TradingEnv(gym.Env):\n",
        "    def __init__(self, data, initial_balance=10000):\n",
        "        super(TradingEnv, self).__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.initial_balance = initial_balance\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Action space: Buy, Sell, Hold\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: Open, High, Low, Close, Volume, SMA_20, SMA_50, RSI, MACD\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(9,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Initialize variables\n",
        "        self.balance = initial_balance\n",
        "        self.shares_held = 0\n",
        "        self.net_worth = initial_balance\n",
        "        self.max_net_worth = initial_balance\n",
        "        self.total_shares_bought = 0\n",
        "        self.total_shares_sold = 0\n",
        "        self.total_commission_paid = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = self.initial_balance\n",
        "        self.shares_held = 0\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.max_net_worth = self.initial_balance\n",
        "        self.total_shares_bought = 0\n",
        "        self.total_shares_sold = 0\n",
        "        self.total_commission_paid = 0\n",
        "        self.current_step = 0\n",
        "\n",
        "        return self._next_observation()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        # Get the current row data\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        return np.array([\n",
        "            row['Open'], row['High'], row['Low'], row['Close'], row['Volume'],\n",
        "            row['SMA_20'], row['SMA_50'], row['RSI'], row['MACD']\n",
        "        ])\n",
        "\n",
        "    def step(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "\n",
        "        if action == 0:  # Buy\n",
        "            self._buy_shares(current_price)\n",
        "        elif action == 1:  # Sell\n",
        "            self._sell_shares(current_price)\n",
        "        # Hold does nothing\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        # Calculate the reward\n",
        "        reward = self._calculate_reward()\n",
        "\n",
        "        obs = self._next_observation()\n",
        "\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def _buy_shares(self, current_price):\n",
        "        if self.balance > current_price:\n",
        "            shares_bought = self.balance // current_price\n",
        "            self.balance -= shares_bought * current_price\n",
        "            self.shares_held += shares_bought\n",
        "            self.total_shares_bought += shares_bought\n",
        "\n",
        "    def _sell_shares(self, current_price):\n",
        "        if self.shares_held > 0:\n",
        "            self.balance += self.shares_held * current_price\n",
        "            self.total_shares_sold += self.shares_held\n",
        "            self.shares_held = 0\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        self.net_worth = self.balance + self.shares_held * self.data.iloc[self.current_step]['Close']\n",
        "        profit = self.net_worth - self.initial_balance\n",
        "\n",
        "        # Calculate returns and risk-adjusted return\n",
        "        returns = (self.net_worth - self.initial_balance) / self.initial_balance\n",
        "        risk_adjusted_return = returns / (np.std(returns) if np.std(returns) != 0 else 1)\n",
        "\n",
        "        # Penalty for inactivity\n",
        "        inactivity_penalty = -0.001 if self.shares_held > 0 else 0\n",
        "\n",
        "        # Combine the rewards\n",
        "        reward = (0.7 * profit) + (0.2 * risk_adjusted_return) + (0.1 * inactivity_penalty)\n",
        "        return reward\n",
        "\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        print(f'Step: {self.current_step}')\n",
        "        print(f'Balance: {self.balance}')\n",
        "        print(f'Shares held: {self.shares_held}')\n",
        "        print(f'Net worth: {self.net_worth}')\n",
        "        print(f'Total shares bought: {self.total_shares_bought}')\n",
        "        print(f'Total shares sold: {self.total_shares_sold}')\n"
      ],
      "metadata": {
        "id": "88oJf3QPraOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install 'shimmy>=0.2.1'"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCrhG9TbsHDN",
        "outputId": "5a540216-4beb-47f5-fcff-ee001c3a9325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy>=0.2.1\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.26.4)\n",
            "Collecting gymnasium>=1.0.0a1 (from shimmy>=0.2.1)\n",
            "  Downloading gymnasium-1.0.0a2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy>=0.2.1) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy>=0.2.1) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=1.0.0a1->shimmy>=0.2.1)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Downloading gymnasium-1.0.0a2-py3-none-any.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.3/954.3 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, shimmy\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0a2 shimmy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDvGkJ0lAKzd",
        "outputId": "88d0e5d3-88ae-48f8-e758-adf404ecb302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable_baselines3)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.0.0a2\n",
            "    Uninstalling gymnasium-1.0.0a2:\n",
            "      Successfully uninstalled gymnasium-1.0.0a2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "shimmy 2.0.0 requires gymnasium>=1.0.0a1, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 stable_baselines3-2.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO  # Import PPO\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define directories for logs and best model saving\n",
        "log_dir = \"./ppo_hyperparameter_tuning/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Assuming 'env' and 'test_env' are already defined and initialized\n",
        "\n",
        "# Hyperparameter grid\n",
        "learning_rates = [0.0001, 0.0003, 0.001]\n",
        "clip_ranges = [0.1, 0.2, 0.3]\n",
        "batch_sizes = [64, 128, 256]\n",
        "\n",
        "# Initialize results list\n",
        "results = []\n",
        "best_reward = -float('inf')\n",
        "best_params = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for cr in clip_ranges:\n",
        "        for bs in batch_sizes:\n",
        "            print(f\"Testing with learning_rate={lr}, clip_range={cr}, batch_size={bs}\")\n",
        "\n",
        "            # Define a callback to evaluate and save the best model\n",
        "            eval_callback = EvalCallback(test_env, best_model_save_path=log_dir,\n",
        "                                         log_path=log_dir, eval_freq=5000,\n",
        "                                         deterministic=True, render=False)\n",
        "\n",
        "            # Initialize the PPO model with current hyperparameters\n",
        "            model = PPO('MlpPolicy', env, learning_rate=lr, batch_size=bs, clip_range=cr, verbose=1)\n",
        "\n",
        "            # Train the model\n",
        "            model.learn(total_timesteps=50000, callback=eval_callback)\n",
        "\n",
        "            # Evaluate the model on the test environment\n",
        "            mean_reward, std_reward = evaluate_policy(model.policy, test_env, n_eval_episodes=10)\n",
        "\n",
        "            # Store the result in the list\n",
        "            results.append({\n",
        "                \"learning_rate\": lr,\n",
        "                \"clip_range\": cr,\n",
        "                \"batch_size\": bs,\n",
        "                \"mean_reward\": mean_reward,\n",
        "                \"std_reward\": std_reward\n",
        "            })\n",
        "\n",
        "# Convert the results to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Sort the DataFrame by mean_reward in descending order\n",
        "results_df = results_df.sort_values(by=\"mean_reward\", ascending=False)\n",
        "\n",
        "# Print the top results\n",
        "print(results_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EDE2ahXW2VJl",
        "outputId": "edd84d17-6642-4254-95b2-72156d8c6972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 818          |\n",
            "|    mean_reward          | 0            |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 50000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.108033e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 1.37e+09     |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -4.86e-05    |\n",
            "|    value_loss           | 2.45e+09     |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.38e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 257      |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 198      |\n",
            "|    total_timesteps | 51200    |\n",
            "---------------------------------\n",
            "Testing with learning_rate=0.001, clip_range=0.3, batch_size=128\n",
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 4.32e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 499      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 3.57e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 433          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.184301e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 8.62e+08     |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.000294    |\n",
            "|    value_loss           | 1.84e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 818          |\n",
            "|    mean_reward          | 0            |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 5000         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.278294e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 4.06e+08     |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.000159    |\n",
            "|    value_loss           | 8.84e+08     |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.75e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 301      |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 20       |\n",
            "|    total_timesteps | 6144     |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 3.04e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 327          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.228621e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 8.44e+08     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.0003      |\n",
            "|    value_loss           | 1.44e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 10000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00027311183 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 3.85e+08      |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.000657     |\n",
            "|    value_loss           | 7.84e+08      |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 2.98e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 285      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 10240    |\n",
            "---------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1.69e+03      |\n",
            "|    ep_rew_mean          | 2.8e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 304           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 40            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6327616e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 4.58e+08      |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -7.35e-05     |\n",
            "|    value_loss           | 8.74e+08      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 2.93e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 312          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 45           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.317516e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 2.51e+08     |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.00011     |\n",
            "|    value_loss           | 5.11e+08     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 15000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.4787088e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 1.77e+09      |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -2.8e-05      |\n",
            "|    value_loss           | 3.65e+09      |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.52e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 292      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 56       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 3.35e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 301          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 61           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.965331e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 2.36e+09     |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -1.15e-05    |\n",
            "|    value_loss           | 3.86e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 20000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00017418069 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | -2.38e-07     |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 3.36e+08      |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.000499     |\n",
            "|    value_loss           | 6.81e+08      |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.53e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 284      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 72       |\n",
            "|    total_timesteps | 20480    |\n",
            "---------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1.69e+03      |\n",
            "|    ep_rew_mean          | 3.67e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 293           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 76            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.1624146e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 1.9e+09       |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | 9.73e-06      |\n",
            "|    value_loss           | 3.9e+09       |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 3.68e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 301          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 81           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.078776e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 1.12e+09     |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -9.92e-05    |\n",
            "|    value_loss           | 2.3e+09      |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 25000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.3621056e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 1.01e+09      |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.000451     |\n",
            "|    value_loss           | 2.21e+09      |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.88e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 288      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 92       |\n",
            "|    total_timesteps | 26624    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 3.9e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 293          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 97           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.628514e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 2.45e+09     |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -4.15e-05    |\n",
            "|    value_loss           | 4.6e+09      |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 30000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.4043077e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 2.71e+08      |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -8.56e-05     |\n",
            "|    value_loss           | 4.82e+08      |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.74e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 284      |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 30720    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 3.63e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 289          |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 113          |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.371666e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 7.25e+08     |\n",
            "|    n_updates            | 150          |\n",
            "|    policy_gradient_loss | -3.7e-05     |\n",
            "|    value_loss           | 1.46e+09     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1.69e+03      |\n",
            "|    ep_rew_mean          | 3.55e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 294           |\n",
            "|    iterations           | 17            |\n",
            "|    time_elapsed         | 118           |\n",
            "|    total_timesteps      | 34816         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00040562564 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 1.42e+08      |\n",
            "|    n_updates            | 160           |\n",
            "|    policy_gradient_loss | -0.000572     |\n",
            "|    value_loss           | 2.79e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 35000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.1647483e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 6.14e+08      |\n",
            "|    n_updates            | 170           |\n",
            "|    policy_gradient_loss | -0.000174     |\n",
            "|    value_loss           | 1.32e+09      |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.59e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 286      |\n",
            "|    iterations      | 18       |\n",
            "|    time_elapsed    | 128      |\n",
            "|    total_timesteps | 36864    |\n",
            "---------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1.69e+03      |\n",
            "|    ep_rew_mean          | 3.59e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 291           |\n",
            "|    iterations           | 19            |\n",
            "|    time_elapsed         | 133           |\n",
            "|    total_timesteps      | 38912         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.7880964e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 1.13e+09      |\n",
            "|    n_updates            | 180           |\n",
            "|    policy_gradient_loss | -1.52e-05     |\n",
            "|    value_loss           | 2.08e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 40000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.8937036e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 2.43e+08      |\n",
            "|    n_updates            | 190           |\n",
            "|    policy_gradient_loss | -5.46e-05     |\n",
            "|    value_loss           | 5.02e+08      |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.46e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 284      |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 144      |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 3.52e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 287          |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 149          |\n",
            "|    total_timesteps      | 43008        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001344815 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 4.31e+08     |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.000339    |\n",
            "|    value_loss           | 8.47e+08     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 45000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.9234816e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 1.11e+09      |\n",
            "|    n_updates            | 210           |\n",
            "|    policy_gradient_loss | -0.000169     |\n",
            "|    value_loss           | 2.05e+09      |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.52e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 281      |\n",
            "|    iterations      | 22       |\n",
            "|    time_elapsed    | 159      |\n",
            "|    total_timesteps | 45056    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 3.63e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 285          |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 164          |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.834315e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 1.7e+09      |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -4.38e-05    |\n",
            "|    value_loss           | 3.66e+09     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 3.69e+06     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 289          |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 169          |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.461757e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 9.82e+08     |\n",
            "|    n_updates            | 230          |\n",
            "|    policy_gradient_loss | -0.000137    |\n",
            "|    value_loss           | 1.94e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 818          |\n",
            "|    mean_reward          | 0            |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 50000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.829782e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 3.23e+09     |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -6.18e-05    |\n",
            "|    value_loss           | 6.61e+09     |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.8e+06  |\n",
            "| time/              |          |\n",
            "|    fps             | 283      |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 180      |\n",
            "|    total_timesteps | 51200    |\n",
            "---------------------------------\n",
            "Testing with learning_rate=0.001, clip_range=0.3, batch_size=256\n",
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.71e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 518      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.69e+03     |\n",
            "|    ep_rew_mean          | 2.4e+06      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 448          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.718206e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.3          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 6.59e+08     |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -4.79e-05    |\n",
            "|    value_loss           | 1.34e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 5000          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1408207e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 5.17e+08      |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -5.01e-05     |\n",
            "|    value_loss           | 1.19e+09      |\n",
            "-------------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 4.15e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 322      |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 19       |\n",
            "|    total_timesteps | 6144     |\n",
            "---------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1.69e+03      |\n",
            "|    ep_rew_mean          | 4.97e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 342           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 23            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5055994e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 3.3e+09       |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -1.61e-05     |\n",
            "|    value_loss           | 6.89e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 10000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8551655e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 2.15e+09      |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -1.72e-05     |\n",
            "|    value_loss           | 3.81e+09      |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 4.52e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 297      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 34       |\n",
            "|    total_timesteps | 10240    |\n",
            "---------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1.69e+03      |\n",
            "|    ep_rew_mean          | 4.49e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 316           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 38            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5477063e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 6.61e+08      |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -2.72e-05     |\n",
            "|    value_loss           | 1.4e+09       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1.69e+03      |\n",
            "|    ep_rew_mean          | 4.7e+06       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 332           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 43            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.5283675e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 9.9e+08       |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -4.28e-05     |\n",
            "|    value_loss           | 1.93e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 818           |\n",
            "|    mean_reward          | 0             |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 15000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.0667808e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 2e+09         |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -6.33e-05     |\n",
            "|    value_loss           | 3.94e+09      |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 4.53e+06 |\n",
            "| time/              |          |\n",
            "|    fps             | 305      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 53       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1.69e+03      |\n",
            "|    ep_rew_mean          | 4.17e+06      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 316           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 58            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1523662e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.3           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 3.58e+08      |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -7.67e-07     |\n",
            "|    value_loss           | 7.96e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-1148573.09 +/- 0.00\n",
            "Episode length: 818.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 818         |\n",
            "|    mean_reward          | -1.15e+06   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 20000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012037936 |\n",
            "|    clip_fraction        | 0.0563      |\n",
            "|    clip_range           | 0.3         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 9.45e+07    |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00541    |\n",
            "|    value_loss           | 1.81e+08    |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.69e+03 |\n",
            "|    ep_rew_mean     | 3.8e+06  |\n",
            "| time/              |          |\n",
            "|    fps             | 299      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 68       |\n",
            "|    total_timesteps | 20480    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-325cbee2c6f7>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Evaluate the model on the test environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 315\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_features_extractor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             \u001b[0mlatent_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_vf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mpi_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvf_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0mall\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0mare\u001b[0m \u001b[0mshared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlatent_policy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlatent_value\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward_actor\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the results to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Sort the DataFrame by mean_reward in descending order\n",
        "results_df = results_df.sort_values(by=\"mean_reward\", ascending=False)\n",
        "\n",
        "# Sort the DataFrame by mean_reward in descending order\n",
        "results_df = results_df.sort_values(by=\"mean_reward\", ascending=False)\n",
        "\n",
        "# Print the top results\n",
        "print(results_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhmq89L7YyS2",
        "outputId": "3538367a-7b02-4456-d43f-529a563dcab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    learning_rate  clip_range  batch_size  mean_reward  std_reward\n",
            "14         0.0003         0.2         256  2329.860717         0.0\n",
            "1          0.0001         0.1         128     0.000000         0.0\n",
            "6          0.0001         0.3          64     0.000000         0.0\n",
            "20         0.0010         0.1         256     0.000000         0.0\n",
            "21         0.0010         0.2          64     0.000000         0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! It looks like you've successfully identified a set of hyperparameters that yield a good result:\n",
        "\n",
        "Learning Rate: 0.0003\n",
        "Clip Range: 0.2\n",
        "Batch Size: 256\n",
        "Mean Reward: 2329.86\n",
        "Standard Deviation of Reward: 0.0\n",
        "Analysis of the Results\n",
        "Mean Reward of 2329.86: This suggests that the model is achieving a positive average return per episode, which indicates profitable trading behavior in the simulated environment.\n",
        "\n",
        "Standard Deviation of 0.0: The std_reward being 0 suggests that the results are very consistent across the evaluation episodes, meaning that the model performs reliably with the given hyperparameters."
      ],
      "metadata": {
        "id": "lHvKkYI0Zx_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# Wrap the environment\n",
        "env = make_vec_env(lambda: TradingEnv(data_train), n_envs=1)\n",
        "\n",
        "# Instantiate the PPO agent\n",
        "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=\"./ppo_trading_tensorboard_v2/\")\n",
        "\n",
        "\n",
        "\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=100000)  # Increased timesteps\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_trading_model\")\n",
        "\n",
        "# Test the trained model\n",
        "test_env = TradingEnv(data_test)\n",
        "obs = test_env.reset()\n",
        "\n",
        "for i in range(len(data_test) - 1):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, reward, done, info = test_env.step(action)\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Save the actions taken by the model for analysis\n",
        "actions_df = pd.DataFrame({\n",
        "    \"Date\": data_test.index,\n",
        "    \"Action\": action,  # This needs to be logged at each step\n",
        "    \"Close Price\": data_test['Close'],\n",
        "    \"Net Worth\": test_env.net_worth\n",
        "})\n",
        "\n",
        "actions_df.to_csv(\"trading_actions.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8NT7_rSrfeE",
        "outputId": "8380ac22-ea99-49f0-8975-cc8d3f5e8e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable-baselines3)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.0.0a2\n",
            "    Uninstalling gymnasium-1.0.0a2:\n",
            "      Successfully uninstalled gymnasium-1.0.0a2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "shimmy 2.0.0 requires gymnasium>=1.0.0a1, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 stable-baselines3-2.3.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Logging to ./ppo_trading_tensorboard/PPO_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 824       |\n",
            "|    ep_rew_mean     | -6.57e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 447       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 4         |\n",
            "|    total_timesteps | 2048      |\n",
            "----------------------------------\n",
            "--------------------------------------------\n",
            "| rollout/                |                |\n",
            "|    ep_len_mean          | 824            |\n",
            "|    ep_rew_mean          | -7.87e+05      |\n",
            "| time/                   |                |\n",
            "|    fps                  | 365            |\n",
            "|    iterations           | 2              |\n",
            "|    time_elapsed         | 11             |\n",
            "|    total_timesteps      | 4096           |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000112707814 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.1           |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 3.18e+08       |\n",
            "|    n_updates            | 10             |\n",
            "|    policy_gradient_loss | -0.000189      |\n",
            "|    value_loss           | 5.26e+08       |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 824           |\n",
            "|    ep_rew_mean          | -5.37e+05     |\n",
            "| time/                   |               |\n",
            "|    fps                  | 380           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 16            |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2579898e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.42e+08      |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -4.43e-05     |\n",
            "|    value_loss           | 9.01e+08      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 824           |\n",
            "|    ep_rew_mean          | -4.51e+05     |\n",
            "| time/                   |               |\n",
            "|    fps                  | 379           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 21            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.3120014e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.44e+08      |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -1.43e-05     |\n",
            "|    value_loss           | 3.33e+08      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 824          |\n",
            "|    ep_rew_mean          | -5.32e+05    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 382          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001712669 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.6e+07      |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.000154    |\n",
            "|    value_loss           | 1.64e+08     |\n",
            "------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}